{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the feature_extraction.py first!!!!!!!!!!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "from textblob import TextBlob\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv(\"./data/X_train.csv\")\n",
    "submission = pd.read_csv(\"./data/X_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemn the text and summary in X_train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain = pd.DataFrame({'Summary': train['Summary'], 'Text': train['Text']})\n",
    "stmer=SnowballStemmer(\"english\",ignore_stopwords=True)\n",
    "\n",
    "def stem(text):\n",
    "    return ' '.join([stmer.stem(word) for word in word_tokenize(text)])\n",
    "#df['test']=np.nan\n",
    "#tqdm.pandas(desc=\"my bar!\")\n",
    "dftrain['Summary']=dftrain['Summary'].fillna(\"\").apply(stem)\n",
    "dftrain['Test']=dftrain['Text'].fillna(\"\").apply(stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain.to_csv(\"./data/dftrain_c.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemn the text and summary in X_test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsub = pd.DataFrame({'Summary': submission['Summary'], 'Text': submission['Text']})\n",
    "stmer=SnowballStemmer(\"english\",ignore_stopwords=True)\n",
    "\n",
    "def stem(text):\n",
    "    return ' '.join([stmer.stem(word) for word in word_tokenize(text)])\n",
    "#df['test']=np.nan\n",
    "#tqdm.pandas(desc=\"my bar!\")\n",
    "dfsub['Summary']=dfsub['Summary'].fillna(\"\").apply(stem)\n",
    "dfsub['Test']=dfsub['Text'].fillna(\"\").apply(stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsub.to_csv(\"./data/dfsub_c.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('./data/dfsub_c.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine stemmer together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain['Total'] = dftrain['Summary'] + dftrain['Test']\n",
    "\n",
    "dfsub['Total'] = dfsub['Summary'] + \" \" + dfsub['Test']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFID the stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv = TfidfVectorizer(max_df=0.999, min_df=0.005, max_features=10000)\n",
    "train_tfid = tv.fit_transform(dftrain['Total'].apply(lambda x: np.str_(x)))\n",
    "#train_tfdf = pd.DataFrame(train_tfid.toarray(), columns=tv.get_feature_names())\n",
    "train_tfdf = pd.DataFrame(train_tfid.toarray(), columns=tv.get_feature_names_out())\n",
    "train_tfdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sub_tfid = tv.transform(dfsub['Total'].apply(lambda x: np.str_(x)))\n",
    "#Sub_tfdf = pd.DataFrame(Sub_tfid.toarray(), columns=tv.get_feature_names())\n",
    "Sub_tfdf = pd.DataFrame(Sub_tfid.toarray(), columns=tv.get_feature_names_out())\n",
    "Sub_tfdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concate together for further prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analisys of text and summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pop(s):\n",
    "    a,b=TextBlob(s).sentiment\n",
    "    return a\n",
    "\n",
    "train['Pop_text']=train['Text'].fillna(\"\").apply(pop)\n",
    "\n",
    "train['Pop_Summary']=train['Summary'].fillna(\"\").apply(pop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "submission['Pop_text']=submission['Text'].fillna(\"\").apply(pop)\n",
    "\n",
    "submission['Pop_Summary']=submission['Summary'].fillna(\"\").apply(pop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concate together for further prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_num=train.drop(columns=['Id', 'ProductId', 'UserId', 'Text', 'Summary','Score'])\n",
    "train_final=pd.concat([train_num,train_tfdf],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_num=submission.drop(columns=['Id', 'ProductId', 'UserId', 'Text', 'Summary','Score'])\n",
    "sub_final=pd.concat([sub_num,Sub_tfdf],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split for train and test set and build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split( train_final, train['Score'],test_size=0.25,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression(n_jobs=-1).fit(x_train, y_train)\n",
    "Y_predictions = model.predict(x_test)\n",
    "submission['Score'] = model.predict(sub_final)\n",
    "def to_score(score):\n",
    "    if score < 1:\n",
    "        return 1\n",
    "    elif score > 5:\n",
    "        return 5\n",
    "    else:\n",
    "        return round(score)\n",
    "\n",
    "submission['Score'] = submission['Score'].apply(to_score)\n",
    "submission_file = submission[['Id', 'Score']]\n",
    "submission_file.to_csv(\"./data/submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import  mean_squared_error\n",
    "from math import sqrt\n",
    "print(\"RMSE on testing set = \", sqrt(mean_squared_error(y_test , Y_predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the linear regression graph\n",
    "plt.scatter(y_test, Y_predictions)\n",
    "plt.xlabel(\"True Values\")\n",
    "plt.ylabel(\"Predictions\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "73bba51fd674e76868acdb9443b3ee17a48842bb446ce47ca4d20eaa5e52578e"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
